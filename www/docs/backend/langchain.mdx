---
sidebar_position: 1
---

import PackageInstall from '@site/src/components/PackageInstall';

# Langchain

<PackageInstall dependencies={['@trpc-chat-agent/langchain']} />

Setting up LangChain is fairly straightforward

```ts
import { LangChainAgentsBackend, asLangChainMessagesArray } from '@trpc-chat-agent/langchain';
import { initAgents } from '@trpc-chat-agent/core';
import { ChatOpenAI } from 'langchain/chatmodels/openai';

export const ai = initAgents
  .context<typeof createContext>()
  .backend(new LangChainAgentsBackend()) // LangChain adapter
  .create();

// Now initialize your agent with these parameters:
const agent = ai.agent({
  // The tools for your agent
  tools: [myTool1, myTool2],

  // LLM, from LangChain. Must extend BaseChatModel
  // Can either be:
  // - a single model
  // - a map of named models. See section below for more info
  // Here's a single model:
  llm: new ChatOpenAI({ model: 'gpt-4o' }),
  // Here's a map of named models:
  llm: {
    chatgpt: new ChatOpenAI({ model: 'gpt-4o' }),
    claude: new ChatOpenAI({ model: 'claude-3-5-sonnet' }),
  },


  // (optional) The maximum update rate for client args and progress indicators
  // defaults to 100ms
  debounceMs: 100,

  // (optional) Implement a custom message transformer. `ctx` is the tRPC context.
  // Uses the implementation shown here if you don't provide one
  transformMessages: async ({ conversation, path, ctx }) => {
    return asLangChainMessagesArray(conversation, path);
  },

  // (optional) System message to append at the beginning. Gets
  // appended after transformMessages runs
  systemMessage: 'You are a helpful assistant',

  // (optional) LangChain callbacks, e.g. for custom tracing
  langchainCallbacks: [customTracingCallback],
});
```

When defining tool calls, you have access to LangChain's extra function parameters:
- Callbacks
- Runnable config

You can treat the `run` call the same as the callbacks in LangChain's `DynamicStructuredTool`.

```ts
const expertModel = new ChatOpenAI({ model: 'gpt-6-pro' });

const myTool = ai.tool({
  name: 'callAgent',
  description: 'Ask an AI agent for help',
  schema: z.object({
    question: z.string(),
  }),

  // highlight-start
  // When calling `run`, the tool has access to LangChain's:
  // - Callbacks
  // - Runnable config
  run: async ({ input, /* ... */ }, langchainCallbacks, runnableConfig) => {
  // highlight-end
    // Invoke other LangChain APIs
    const chatResult = await expertModel.call({
      messages: [new UserMessage(input.question)],
    });
    const result = chatResult.content;

    return {
      response: result,
      clientResult: null,
    };
  },
});
```

## Model selection

When multiple models are provided to the `llm` field, a new field called `selectedLlm` is added to invocation args. The field would enforce selection of a model.

For example:

```ts
function ChatComponent({ id, onUpdateConversationId }: ChatComponentProps) {
  const [input, setInput] = useState('');

  const { messages, beginMessage } = useConversation({
    initialConversationId: id,
    onUpdateConversationId: onUpdateConversationId,
    router: trpcClient.chat,
  });

  const handleSubmit = (message: string) => {
    if (message.trim()) {
      beginMessage({
        userMessage: message,
        // highlight-start
        invokeArgs: { selectedLlm: 'chatgpt' },
        // highlight-end
      });
      setInput('');
    }
  };

  // ...
}
```

## Anthropic Prompt Caching

Anthropic models require custom input parameters to enable prompt caching. `@trpc-chat-agent/langchain` automatically adds those parameters when the model is detected as a ChatAnthropic instance.

The `cache_control: { type: 'ephemeral' }` parameters are added in the following locations:
- The last tool
- The system message
- The second last message
- The last message

You can control which of these are included with the `anthropicCacheLevel` option in the agent definition.

For example:

```ts
const agent = ai.agent({
  // ...
  llm: new ChatAnthropic({ model: 'claude-3-5-sonnet-20241022' }),
  anthropicCacheLevel: AnthropicCacheLevels.UpToSystemMessage, // Only cache tools and system message
  // ...
});
```
