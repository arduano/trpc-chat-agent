---
sidebar_position: 1
---

import PackageInstall from '@site/src/components/PackageInstall';

# Langchain

<PackageInstall dependencies={['@trpc-chat-agent/langchain']} />

Setting up LangChain is fairly straightforward

```ts
import { langchainBackend, asLangChainMessagesArray } from '@trpc-chat-agent/langchain';
import { initAgents } from '@trpc-chat-agent/core';
import { ChatOpenAI } from 'langchain/chatmodels/openai';

export const ai = initAgents
  .context<typeof createContext>()
  .backend(langchainBackend) // LangChain adapter
  .create();

// Now initialize your agent with these parameters:
const agent = ai.agent({
  // The tools for your agent
  tools: [myTool1, myTool2],

  // LLM, from LangChain. Must extend BaseChatModel
  // Can either be:
  // - a single model
  // - a function to select what model to use based on extra args
  // Here's a single model:
  llm: new ChatOpenAI({ model: 'gpt-4o' }),
  // Here's a map of named models:
  llm: ({ extraArgs }) => new ChatOpenAI({ model: extraArgs.myChosenModelName }),


  // (optional) The maximum update rate for client args and progress indicators
  // defaults to 100ms
  debounceMs: 100,

  // (optional) Implement a custom message transformer. `ctx` is the tRPC context.
  // Uses the implementation shown here if you don't provide one
  transformMessages: async ({ conversation, path, ctx }) => {
    return asLangChainMessagesArray(conversation, path);
  },

  // (optional) System message to append at the beginning. Gets
  // appended after transformMessages runs
  systemMessage: 'You are a helpful assistant',

  // (optional) LangChain callbacks, e.g. for custom tracing
  langchainCallbacks: [customTracingCallback],
});
```

When defining tool calls, you have access to LangChain's extra function parameters:
- Callbacks
- Runnable config

You can treat the `run` call the same as the callbacks in LangChain's `DynamicStructuredTool`.

```ts
const expertModel = new ChatOpenAI({ model: 'gpt-6-pro' });

const myTool = ai.tool({
  name: 'callAgent',
  description: 'Ask an AI agent for help',
  schema: z.object({
    question: z.string(),
  }),

  // highlight-start
  // When calling `run`, the tool has access to LangChain's:
  // - Callbacks
  // - Runnable config
  run: async ({ input, /* ... */ }, langchainCallbacks, runnableConfig) => {
  // highlight-end
    // Invoke other LangChain APIs
    const chatResult = await expertModel.call({
      messages: [new UserMessage(input.question)],
    });
    const result = chatResult.content;

    return {
      response: result,
      clientResult: null,
    };
  },
});
```

## Anthropic Prompt Caching

Anthropic models require custom input parameters to enable prompt caching. `@trpc-chat-agent/langchain` automatically adds those parameters when the model is detected as a ChatAnthropic instance.

The `cache_control: { type: 'ephemeral' }` parameters are added in the following locations:
- The last tool
- The system message
- The second last message
- The last message

You can control which of these are included with the `anthropicCacheLevel` option in the agent definition.

For example:

```ts
const agent = ai.agent({
  // ...
  llm: new ChatAnthropic({ model: 'claude-3-5-sonnet-20241022' }),
  anthropicCacheLevel: AnthropicCacheLevels.UpToSystemMessage, // Only cache tools and system message
  // ...
});
